[
  {
    "objectID": "Day1.html",
    "href": "Day1.html",
    "title": "Day 1",
    "section": "",
    "text": "Day 1 - The Digit Recognizer (04.03.2023)\nToday I start with my first projet, the Digit Recognizer, a challenge from Kaggle for beginners. On Kaggle only csv files are uploaded, but I want to learn how to use the original source material, so I download the .gz files from http://yann.lecun.com/exdb/mnist/\nSince the files are stored in binary format, I first need to make them usable for me, to do this I use gzip to read the contents of the .gz files and struct to unzip these files.\n\nimport os\nimport gzip\nimport numpy as np\nimport struct\nimport urllib.request\n\ndef download_mnist(url, filename):\n    if not os.path.exists(filename):\n        print(f\"Downloading {filename}...\")\n        urllib.request.urlretrieve(url, filename)\n        print(f\"Download complete.\")\n\n# Download the dataset\nmnist_url = \"http://yann.lecun.com/exdb/mnist/\"\ntrain_images = \"train-images-idx3-ubyte.gz\"\ntrain_labels = \"train-labels-idx1-ubyte.gz\"\ntest_images = \"t10k-images-idx3-ubyte.gz\"\ntest_labels = \"t10k-labels-idx1-ubyte.gz\"\n\ndownload_mnist(mnist_url + train_images, train_images)\ndownload_mnist(mnist_url + train_labels, train_labels)\ndownload_mnist(mnist_url + test_images, test_images)\ndownload_mnist(mnist_url + test_labels, test_labels)\n\n# Import the dataset\ndef load_mnist(images_path, labels_path):\n    with gzip.open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('>II', lbpath.read(8))\n        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n\n    with gzip.open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n\n    return images, labels\n\nX_train, y_train = load_mnist(\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\")\nX_test, y_test = load_mnist(\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\")\n\nNow all that remains is to convert the data into the correct 28x28 pixel form.\n\nX_train = X_train.reshape(-1, 28, 28, 1)\nX_test = X_test.reshape(-1, 28, 28, 1)\n\nMy next step is the definition of the model. I know that Convolutional Neural Networks are used for image recognition, so I will do that here too. I start with two convolutional layers and two dense layers and see if I can get it to work. According to the internet it is normal to start with a filter count of 32 in the first conv layer, so I do that here too, followed by a max pooling layer with stride (2,2), which is also standard. I use he_normal as kernel_initalizer, as it is supposed to work well with “relu”.\n\nfrom tensorflow.keras import layers, models\n\nimg_width = 28\nimg_height = 28\nnum_channels = 1\n\n# Inputs to the model\ninput_img = layers.Input(shape=(img_width, img_height, num_channels), \n                         name=\"image\", \n                         dtype=\"float32\"\n                         )\n\n# First conv block\nx = layers.Conv2D(32,\n                  (3, 3),\n                  activation=\"relu\",\n                  kernel_initializer=\"he_normal\",\n                  name=\"Conv1\",\n                  )(input_img)\n\n# First Max Pooling Layer\nx = layers.MaxPooling2D((2, 2), name=\"Pool1\")(x)\n\n# Second conv block\nx = layers.Conv2D(64,\n                  (3, 3),\n                  activation=\"relu\",\n                  kernel_initializer=\"he_normal\",\n                  name=\"Conv2\",\n                  )(input_img)\n\n# Second Max Pooling Layer\nx = layers.MaxPooling2D((2, 2), name=\"Pool2\")(x)\n\nAfter the Conv part comes a Flatten layer, so that the output can be used as input for the following Dense layer.\n\n# Flatten the input\nx = layers.Flatten()(x)\n\nThe use of two dense layers in a deep learning model is common in many tasks, including image classification. The first dense layer is used to learn a high-level representation of the input data, while the second dense layer is used to make the final prediction. The ‘softmax’ activation function is commonly used in the output layer of a deep learning model for classification tasks, as it converts the outputs of the model into a probability distribution over the classes. The 10 in the Dense Layer is because I have 10 digits. The Adam optimizer is a popular choice in deep learning, so I go with that. “sparse_categorical_crossentropy” is a loss function used in multi-class classification problems where each sample has a single true label and the goal is to predict the class labels correctly. Accuracy is pretty standard as metric, so I use that here.\n\n# First Dense Layer\nx = layers.Dense(64, activation=\"relu\", name=\"Dense1\")(x)\n\n# Second Dense Layer\noutput = layers.Dense(10, activation=\"softmax\", name=\"Dense2\")(x)\n\n# Define the model\nmodel = models.Model(inputs=input_img, \n                           outputs=output, \n                           name=\"Digit_Model\"\n                           )\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)\n\npredictions = model.predict(X_test)\n\nFor a first try this looks pretty good, we got an accuracy of 96.2%!\nAt the end, I want to look at where the model failed, for that we’ll look at it with matplotlib:\n\nimport matplotlib.pyplot as plt\n\n# Get the predicted classes for each sample in X_test\npredicted_classes = np.argmax(predictions, axis=1)\n\n# Set a threshold for the prediction uncertainty\nuncertainty_threshold = 0.9\n\n# Get the indices of the samples with prediction uncertainty above the threshold\nuncertain_indices = np.where(np.max(predictions, axis=1) < uncertainty_threshold)[0]\n\nnum_images = 70\n\nwrong_indices = [i for i, prediction in enumerate(predicted_classes) if prediction != y_test[i]]\nrows = num_images // 10 + (num_images % 10 != 0)\nfig, axs = plt.subplots(rows, 10, figsize=(15, 15), subplot_kw={'xticks': [], 'yticks': []})\nfig.subplots_adjust(hspace=0.5, wspace=0.05)\naxs = axs.ravel()\nfor i, index in enumerate(wrong_indices[:num_images]):\n    axs[i].imshow(X_test[index].reshape(28, 28), cmap=\"gray\")\n    axs[i].set_title(\"True: {}\\nPredicted: {}\".format(y_test[index], predicted_classes[index]))\nfor i in range(num_images, rows * 10):\n    fig.delaxes(axs[i])\nplt.show()\n\nWith some of them you can understand why the images were wrongly classified, but others look quite clear. Now we have to do some fine tuning, I will do that in the next blog entry."
  },
  {
    "objectID": "myblog/index.html",
    "href": "myblog/index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Welcome to My Blog\nThis is the home page of my blog."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Welcome to My Blog\nThis is the home page of my blog."
  },
  {
    "objectID": "myblog/Post2.html",
    "href": "myblog/Post2.html",
    "title": "The Digit Recognizer in Pytorch",
    "section": "",
    "text": "In the first blog post I used Keras to implement the Digit Recognizer. Pytorch is another great framework I want to try out, so here is the Digit Recognizer from the last post implemented with Pytorch.\nThe first part for importing the images we can take over from the first project: (As a little improvement I use the reshape in the load_mnist function now, so I can skip the two reshape lines later on).\n\nimport os\nimport gzip\nimport numpy as np\nimport struct\nimport urllib.request\n\ndef download_mnist(url, filename):\n    if not os.path.exists(filename):\n        print(f\"Downloading {filename}...\")\n        urllib.request.urlretrieve(url, filename)\n        print(f\"Download complete.\")\n\n# Download the dataset\nmnist_url = \"http://yann.lecun.com/exdb/mnist/\"\ntrain_images = \"train-images-idx3-ubyte.gz\"\ntrain_labels = \"train-labels-idx1-ubyte.gz\"\ntest_images = \"t10k-images-idx3-ubyte.gz\"\ntest_labels = \"t10k-labels-idx1-ubyte.gz\"\n\ndownload_mnist(mnist_url + train_images, train_images)\ndownload_mnist(mnist_url + train_labels, train_labels)\ndownload_mnist(mnist_url + test_images, test_images)\ndownload_mnist(mnist_url + test_labels, test_labels)\n\n# Import the dataset\ndef load_mnist(images_path, labels_path):\n    with gzip.open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('&gt;II', lbpath.read(8))\n        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n\n    with gzip.open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack(\"&gt;IIII\", imgpath.read(16))\n        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 1, 28, 28)\n\n    return images, labels\n\nX_train, y_train = load_mnist(\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\")\nX_test, y_test = load_mnist(\"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\")\n\nNow we come to the first difference: The data is converted to PyTorch tensors so we can use it with the PyTorch framework.\nI specify the types, as being explicit about the data type helps prevent bugs and ensures that the tensors are compatible with other PyTorch functionalities.\nHere I use “float32” for the images, as most neural network frameworks use 32-bit floating point numbers for the weights and computations to balance computational efficiency with precision. Therefore, input data is often converted to float32 as well. Some hardware accelerators are optimized for float32 arithmetic and might be less efficient if other data types are used.\nThen I use long for the labels, because if I am using a loss function that expects class labels as integers (like Cross-Entropy Loss), the labels must be of type long. If they are not of type long, PyTorch will throw an error.\nAfterwards I use “TensorDataset” to wrap the tensors in a dataset object. This dataset object is then passed to a data loader to create an iterable over the dataset, which is usually used if you have multiple arrays you want to access in pairs.\nThe “DataLoader” object helps manage batching, shuffling, and other dataset iteration functions that can be cumbersome to implement manually.\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n# DataLoader\ntrain_data = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n\ntest_data = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_data, batch_size=32, shuffle=True)\n\nNext we define the device, preferably the GPU with the CPU as a fallback (the easy GPU/CPU switch is a nice feature of Pytorch; transferring our model and data between CPU and GPU in PyTorch is as simple as .to(device), making it convenient for optimizing computational resources).\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nNow we come to the model structure. A warning beforehand: Keras is designed to be user-friendly, modular, and easy to extend, whereas PyTorch is designed to be flexible and expressive, offering more control, but it’s structure is not as intuitive as Keras.\nIn Keras layers are defined and connected in a single flow and the forward pass is implicitly defined by how we connect layers. In PyTorch layers are defined in the init method but are not connected and the forward pass is explicitly defined in the forward method.\nAs PyTorch is more object-oriented than Keras, the model is defined as a class. Generally, a class allows me to encapsulate data and functions that operate on that data within a single entity.\nIn init we therefore first define all layers and their structures and in forward we then only have to put these layers in the correct order and only need a single input. I personally find this structuring clearer than with Keras.\nOnly torch.flatten still needs an additional input. Why? Let’s have a look at the documentation: torch.flatten(input, start_dim=0, end_dim=-1). In Deep Learning we often work with batches, for example if we have 32 greyscale (channels = 1) images with the dimensions 28x28 pixels, then the input tensor has the shape [32,1,28,28] When flattening, you usually want to maintain this batch dimension and flatten only the other dimensions. This is what the start_dim argument allows us to do. For example, setting start_dim=1 will keep the batch dimension intact and flatten all the other dimensions.\nThen [32,1,28,28] becomes [32, 1⋅28⋅28] = [32, 784]. Therefore we set start_dim=1 here.\nAlso I added dropout layers, which help in preventing overfitting and are usually beneficial in larger models. Common dropout rates lie between 0.2 and 0.5. A rate of 0.5 means that approximately half of the input units will be dropped out during training. Higher dropout rates (0.4,0.5) are often used for layers that have more parameters to prevent overfitting. Lower rates (0.1,0.2) are used for layers with fewer parameters.\nThe first dropout layer has a rate of 0.25, which is a moderate dropout rate often used after convolutional layers. This helps to prevent overfitting while not being too aggressive in dropping out neurons, as convolutional layers are generally less prone to overfitting compared to fully connected layers.\nThe second dropout layer has a rate of 0.5, which is a common choice for dropout rates in fully connected layers. This is more aggressive and is intended to prevent overfitting in the more parameter-heavy fully connected layers.\nLastly, you can see that the softmax function also has an additional input. The dim argument in the softmax function specifies the dimension along which the softmax function will be applied. In the end we have a tensor of the size [batch_size,num_classes]. Setting dim=1 applies softmax across each row, giving us the class probabilities for each individual sample in the batch instead of applying softmax independently across each column.\n\nimport torch.nn as nn\n\nclass DigitModel(nn.Module):\n    def __init__(self):\n        super(DigitModel, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        \n        # Activation layers\n        self.relu = nn.ReLU()\n        \n        # Pooling layers\n        self.maxpool = nn.MaxPool2d(kernel_size=2)\n        \n        # Dropout layers\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        # Fully connected layers\n        self.fc1 = nn.Linear(1600, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2(x))\n        x = self.maxpool(x)\n        \n        x = self.dropout1(x)\n\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        \n        x = self.dropout2(x)\n        \n        x = self.fc2(x)\n        \n        return nn.functional.log_softmax(x, dim=1)\n\nThe next step is the implementation of the training loop. Here we choose the same loss and optimizer settings as in the Keras code. Keras has provided us with some information about the process during the training. To get the same information here, we have to help ourselves a little bit. We implement this among other things with the module tqdm for the progress bar (or rather tqdm.notebook, so that it looks nicer in Jupyter Notebook and doesn’t take up so much space).\n\nimport torch.optim as optim\nfrom tqdm.notebook import tqdm\n\n# model = DigitModel()\nmodel = DigitModel().to(device)\n\n# Define the loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())\nnum_epochs = 20\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0\n    correct_predictions = 0\n    \n    # Initialize a progress bar for training\n    with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False) as pbar:\n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # Accumulate the loss and number of correct predictions\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n\n            pbar.update()\n            pbar.set_postfix({'Loss': running_loss / (i + 1), 'Acc': correct_predictions \n                              / ((i + 1) * train_loader.batch_size)})\n\n    # Validation loop\n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0.0\n    val_correct = 0\n    \n    # Initialize a progress bar for validation\n    with tqdm(total=len(test_loader), desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", leave=False) as pbar:\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n\n                pbar.update()\n                pbar.set_postfix({'Val Loss': val_loss / len(test_loader), 'Val Acc': val_correct \n                                  / len(test_loader.dataset)})\n\n    print(f'Epoch {epoch+1}/{num_epochs} | '\n      f'Train Loss: {running_loss / len(train_loader):.4f}, '\n      f'Train Acc: {correct_predictions / len(train_loader.dataset):.4f} | '\n      f'Val Loss: {val_loss / len(test_loader):.4f}, '\n      f'Val Acc: {val_correct / len(test_loader.dataset):.4f}')\nprint('-' * 10)\nprint('Finished Training')\n\n# Testing\ncorrect = 0\ntotal = 0\npredicted_classes = []\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        predicted_classes.extend(predicted.cpu().numpy())\n        \nprint('Accuracy of the network on test images: %.2f %%' % (100 * correct / total))\n\n\n\n\n\n\n\nEpoch 1/20 | Train Loss: 0.5860, Train Acc: 0.8229 | Val Loss: 0.0812, Val Acc: 0.9764\nEpoch 2/20 | Train Loss: 0.2357, Train Acc: 0.9281 | Val Loss: 0.0685, Val Acc: 0.9816\nEpoch 3/20 | Train Loss: 0.1835, Train Acc: 0.9438 | Val Loss: 0.0489, Val Acc: 0.9858\nEpoch 4/20 | Train Loss: 0.1535, Train Acc: 0.9534 | Val Loss: 0.0462, Val Acc: 0.9855\nEpoch 5/20 | Train Loss: 0.1418, Train Acc: 0.9584 | Val Loss: 0.0592, Val Acc: 0.9849\nEpoch 6/20 | Train Loss: 0.1283, Train Acc: 0.9623 | Val Loss: 0.0443, Val Acc: 0.9869\nEpoch 7/20 | Train Loss: 0.1190, Train Acc: 0.9650 | Val Loss: 0.0475, Val Acc: 0.9863\nEpoch 8/20 | Train Loss: 0.1094, Train Acc: 0.9678 | Val Loss: 0.0403, Val Acc: 0.9886\nEpoch 9/20 | Train Loss: 0.1078, Train Acc: 0.9687 | Val Loss: 0.0384, Val Acc: 0.9891\nEpoch 10/20 | Train Loss: 0.0982, Train Acc: 0.9717 | Val Loss: 0.0391, Val Acc: 0.9892\nEpoch 11/20 | Train Loss: 0.0938, Train Acc: 0.9723 | Val Loss: 0.0372, Val Acc: 0.9891\nEpoch 12/20 | Train Loss: 0.0904, Train Acc: 0.9738 | Val Loss: 0.0356, Val Acc: 0.9904\nEpoch 13/20 | Train Loss: 0.0856, Train Acc: 0.9748 | Val Loss: 0.0480, Val Acc: 0.9877\nEpoch 14/20 | Train Loss: 0.0794, Train Acc: 0.9774 | Val Loss: 0.0437, Val Acc: 0.9908\nEpoch 15/20 | Train Loss: 0.0866, Train Acc: 0.9749 | Val Loss: 0.0348, Val Acc: 0.9905\nEpoch 16/20 | Train Loss: 0.0806, Train Acc: 0.9770 | Val Loss: 0.0444, Val Acc: 0.9898\nEpoch 17/20 | Train Loss: 0.0821, Train Acc: 0.9765 | Val Loss: 0.0407, Val Acc: 0.9891\nEpoch 18/20 | Train Loss: 0.0806, Train Acc: 0.9767 | Val Loss: 0.0368, Val Acc: 0.9899\nEpoch 19/20 | Train Loss: 0.0804, Train Acc: 0.9778 | Val Loss: 0.0390, Val Acc: 0.9904\nEpoch 20/20 | Train Loss: 0.0753, Train Acc: 0.9779 | Val Loss: 0.0371, Val Acc: 0.9901\n----------\nFinished Training\nAccuracy of the network on test images: 99.01 %\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first thing that catches the eye is that validation loss and accuracy are always better than training loss and accuracy. This is somewhat counterintuitive, but can result from the dropout during training, which randomly sets a fraction of the input units to 0 at each forward pass. This is a form of regularization and can make the network appear to perform worse during training. During evaluation, dropout is deactivated, so the model uses all the learned features, often resulting in better performance.\nWe also see that the validation accuracy is already better after the first epoch than in the Keras model after 20 epochs, which may also be due to the dropout. In the end we have a test accuracy of over 99%, a 3% increase in comparison to the last blog post!\nI have to say that PyTorch convinces me more than Keras. Apart from the fact that PyTorch seems to prevail over Keras (and TensorFlow too?) especially in scientific applications, I like the model structure better. Also, there seems to be a better ecosystem for PyTorch than for Keras, which includes specialized libraries for various tasks (e.g., torchvision for computer vision, torchaudio for audio tasks, torchtext for text-based applications). There are certainly many other differences, but these are the important ones for me for now, so I will stick with PyTorch.\nLastly I am addding the plot of wrongly determined numbers as in the last blog post.\n\nimport matplotlib.pyplot as plt\n\npredicted_classes = np.array(predicted_classes)\n\n# Set a threshold for the prediction uncertainty\nuncertainty_threshold = 0.9\n\n# Get the indices of the samples with prediction uncertainty above the threshold\nuncertain_indices = np.where(np.max(predicted.cpu().numpy()) &lt; uncertainty_threshold)[0]\n# uncertain_indices = np.where(np.max(predicted_classes) &lt; uncertainty_threshold)[0]\n\nnum_images = 70\n\nwrong_indices = [i for i, prediction in enumerate(predicted_classes) if prediction != y_test[i]]\nrows = num_images // 10 + (num_images % 10 != 0)\nfig, axs = plt.subplots(rows, 10, figsize=(15, 15), subplot_kw={'xticks': [], 'yticks': []})\nfig.subplots_adjust(hspace=0.5, wspace=0.05)\naxs = axs.ravel()\nfor i, index in enumerate(wrong_indices[:num_images]):\n    axs[i].imshow(X_test[index].reshape(28, 28), cmap=\"gray\")\n    axs[i].set_title(\"True: {}\\nPredicted: {}\".format(y_test[index], predicted_classes[index]))\nfor i in range(num_images, rows * 10):\n    fig.delaxes(axs[i])\nplt.show()"
  }
]